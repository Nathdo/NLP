{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68e3023-0531-4d7b-b8e3-7b793339819d",
   "metadata": {},
   "source": [
    "# LDA - Latent Dirichlet Allocation (Clustering for NLP)\n",
    "\n",
    "**LDA is a topic modeling technique that helps uncover hidden themes or \"topics\" present in a large corpus of texts. LDA is a generative probabilistic model that imagines each document as a collection of topics in a certain proportion, and each topic as a collection of words.**\n",
    "\n",
    "**In practice, LDA helps organize and understand large collections of documents by grouping texts based on their thematic similarities. Each topic discovered by LDA can be represented by a set of words that are frequently associated with it, allowing users to grasp the main content of a text or corpus without requiring exhaustive manual reading.**\n",
    "\n",
    "**This model is particularly useful in areas such as document classification, article recommendation, and even for improving search systems by providing insights into the latent structure of text data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a685c63d-a0e3-48c6-9442-748760244200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "data = pd.read_csv('/Users/nathan/Desktop/UPDATED_NLP_COURSE/05-Topic-Modeling/npr.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93ba041-3d9b-461d-a367-81f0d0affed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df = 0.9,  # ignorer les mots qui apparaissent dans 90% des documents\n",
    "                    min_df = 2,\n",
    "                    stop_words = 'english')     # apparait dans au moins deux documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0c961d-13b3-4746-b31c-74e7604cb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = cv.fit_transform(data.Article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b93dca-08a9-40f9-836e-023c81590391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components = 7,    # total subjects\n",
    "                                random_state = 42)\n",
    "\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd598b05-8ed1-4565-83bb-1cf88465a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_words = LDA.components_[0].argsort()[-10:]   # last 10 values of argsort (index)\n",
    "\n",
    "for index in top_ten_words:\n",
    "    print(cv.get_feature_names_out()[index])  # les 10 mots les plus probables du premier sujet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf702f6e-5fdb-4d49-bcb3-19d8b4205d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(LDA.components_)):\n",
    "    top_ten_words = LDA.components_[i].argsort()[-5:] \n",
    "    print(f'The top 5 words for the Topic {i}:', end = '\\n')\n",
    "    lst_word = []\n",
    "    for index in top_ten_words:\n",
    "        lst_word.append(cv.get_feature_names_out()[index])\n",
    "    print(lst_word)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb0ada-f81e-48cf-89a5-65c9fdefc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = LDA.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de18ff01-3940-410d-9ebe-ef2b3281c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_topics = [topics[i].round(2).argmax() for i in range(len(data))]  #  # taking the hight probability to be a part of each topic\n",
    "\n",
    "topics = pd.DataFrame(lst_topics, columns = ['Topics'])\n",
    "data = pd.concat((data, topics), axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29a11d-6333-424c-8eb2-a285ad81f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lst_random = [1, 4, 5, 6, 2, 3]\n",
    "np.array(lst_random).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf373bff-6d8b-41a9-b30b-79080a95c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_words = []\n",
    "\n",
    "for i in range(LDA.n_components):\n",
    "    top_5_words.append(list(LDA.components_[i].argsort()[-5:]))\n",
    "    \n",
    "top_5_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb451aa-ed26-488f-be3c-ed09d80cbff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_final = {}\n",
    "\n",
    "for i, index in enumerate(top_5_words):\n",
    "    dict_final[i] = list(cv.get_feature_names_out()[index])\n",
    "dict_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd9101-86ee-4f32-8f5b-46680578c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Subjects'] = data.Topics.apply(lambda x: dict_final.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd981351-9368-4001-8ff4-0fe094d2c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
